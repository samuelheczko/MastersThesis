{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we run thought the data aquisitin pipeline to get derive results discussed in Samuel Heczko's masters thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import connectome\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import regresson\n",
    "\n",
    "import sys; sys.path\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the connectomes from preregistered fmri images\n",
    "In the thesis we largely conducted analysis on thethe Amsterdam Open MRI Collection (AOMIC). We used special preprocessed files and the data proveided by the AOIMIC people. We derive thea analysis in __ steps. All steps are done to all 900 subjects. We loop following proceedure over all atalases considererd.\n",
    "1. Calcluate the time series data for each brain region for each atlas (extract_connectomes.py)\n",
    "2. Compute the connectome of for each subject\n",
    "3. For each atlas save the upper triangle of the connectivity matrix along the information which brain regions are connected in this connection.\n",
    "\n",
    "\n",
    "Let's start with setting up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defime global parameters\n",
    "cluster = False\n",
    "path = 'data/' ##cahnge for cluster\n",
    "correlation_measure = 'correlation' #can be also tangent, partial\n",
    "res_n = 1\n",
    "res = f'{res_n}x{res_n}x{res_n}'\n",
    "correlation_measure='correlation' ##set for calculation of the brain connectome, choose from 'correlation', 'tangent', 'partial' as implemented by nilearn\n",
    "#templateICBM = '/kyb/agks/sheczko/Downloads/MastersThesis/code/data/templates/mni_icbm152_nlin_asym_09c_nifti/mni_icbm152_nlin_asym_09c/mni_icbm152_t1_tal_nlin_asym_09c.nii' ##use the ICBM T1 template\n",
    "\n",
    "\n",
    "##add the data and names of thigs\n",
    "if cluster:\n",
    "    imgs_paths = glob.glob(path + 'func_images/AOMIC/prep_nifti/*.nii') ##load up a subset of the subejct images\n",
    "\n",
    "else:\n",
    "    imgs_paths = glob.glob(path + 'func_images/AOMIC/prep_nifti/*000*.nii') ##load up all the subejct images\n",
    "subjects_idxs = []\n",
    "for s_n in imgs_paths:\n",
    "    subjects_idxs.append(s_n.split('_')[-1].split('.')[0]) #split the path to extract only the number of the subject from path\n",
    "\n",
    "\n",
    "\n",
    "atlases = glob.glob(path + '/atlases/lawrance2021/label/Human/ICBM/*.nii.gz') ##get the atlases\n",
    "anatomical_labels = glob.glob(path + '/atlases/lawrance2021/label/Human/Anatomical-labels-csv/*.csv') #get the anatomical labels (where available)\n",
    "anatomical_label_names = []\n",
    "for a_l in anatomical_labels:\n",
    "    anatomical_label_names.append(a_l.split('/')[-1].split('.')[0]) #split the path to extract only the name of the atlas\n",
    "\n",
    "\n",
    "##loop over atlases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0008', '0005', '0003', '0006', '0002', '0004', '0001', '0007']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiune with looping over the list of atlases and saving a csv wiht connectivity for all subejcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS00108\n",
      "DS06481\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     ana_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m time_series \u001b[39m=\u001b[39m connectome\u001b[39m.\u001b[39mcalculate_time_series(atlas_path \u001b[39m=\u001b[39m atlas_path,imgs_paths\u001b[39m=\u001b[39mimgs_paths)     \u001b[39m##get the time series of the from all subjects, using the atlas defined\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m correlation_matrices, _ \u001b[39m=\u001b[39m  connectome\u001b[39m.\u001b[39;49mconnectome(time_series \u001b[39m=\u001b[39;49m time_series,correlation_measure\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcorrelation\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m#get the connectivity matrices\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df_ \u001b[39m=\u001b[39m connectome\u001b[39m.\u001b[39msave_connectomes_df(correlation_matrices,anatomical_label_presence \u001b[39m=\u001b[39m al_p, anatomic_labels \u001b[39m=\u001b[39m ana_labels,path_to_save \u001b[39m=\u001b[39m path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m'\u001b[39m, atlas_name \u001b[39m=\u001b[39m atlas_name, n_subjects \u001b[39m=\u001b[39m correlation_matrices\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], correlation_measure \u001b[39m=\u001b[39m correlation_measure,subject_ixds \u001b[39m=\u001b[39m subjects_idxs)\n",
      "File \u001b[0;32m~/Downloads/MastersThesis/code/connectome.py:29\u001b[0m, in \u001b[0;36mconnectome\u001b[0;34m(time_series, correlation_measure)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnectome\u001b[39m(time_series,correlation_measure \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[39m##INPUT: the time series on each brain region as list(?) of arrays (for each participant) given by nilearn, the desired correlation type (choose from linear correaltin, partial correlation, tangent correlation)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m##OUTPUT: signal (standartised)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     connectome_measure \u001b[39m=\u001b[39m ConnectivityMeasure(kind \u001b[39m=\u001b[39m correlation_measure)\n\u001b[0;32m---> 29\u001b[0m     correlation_matrices \u001b[39m=\u001b[39m connectome_measure\u001b[39m.\u001b[39;49mfit_transform(time_series) \n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m correlation_matrices,connectome_measure\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:569\u001b[0m, in \u001b[0;36mConnectivityMeasure.fit_transform\u001b[0;34m(self, X, y, confounds)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    565\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTangent space parametrization can only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbe applied to a group of subjects, as it returns \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdeviations to the mean. You provided \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m X\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, do_fit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, do_transform\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    570\u001b[0m                            confounds\u001b[39m=\u001b[39;49mconfounds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36mConnectivityMeasure._fit_transform\u001b[0;34m(self, X, do_transform, do_fit, confounds)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_estimator_\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39m_standardize(x, detrend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, standardize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcov_estimator_\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39;49m_standardize(x, detrend\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, standardize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:545\u001b[0m, in \u001b[0;36mLedoitWolf.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[39mwith\u001b[39;00m config_context(assume_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 545\u001b[0m     covariance, shrinkage \u001b[39m=\u001b[39m ledoit_wolf(\n\u001b[1;32m    546\u001b[0m         X \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocation_, assume_centered\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, block_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrinkage_ \u001b[39m=\u001b[39m shrinkage\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_covariance(covariance)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:397\u001b[0m, in \u001b[0;36mledoit_wolf\u001b[0;34m(X, assume_centered, block_size)\u001b[0m\n\u001b[1;32m    394\u001b[0m     _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    396\u001b[0m \u001b[39m# get Ledoit-Wolf shrinkage\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m shrinkage \u001b[39m=\u001b[39m ledoit_wolf_shrinkage(\n\u001b[1;32m    398\u001b[0m     X, assume_centered\u001b[39m=\u001b[39;49massume_centered, block_size\u001b[39m=\u001b[39;49mblock_size\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m emp_cov \u001b[39m=\u001b[39m empirical_covariance(X, assume_centered\u001b[39m=\u001b[39massume_centered)\n\u001b[1;32m    401\u001b[0m mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mtrace(emp_cov)) \u001b[39m/\u001b[39m n_features\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:315\u001b[0m, in \u001b[0;36mledoit_wolf_shrinkage\u001b[0;34m(X, assume_centered, block_size)\u001b[0m\n\u001b[1;32m    313\u001b[0m     cols \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(block_size \u001b[39m*\u001b[39m j, block_size \u001b[39m*\u001b[39m (j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    314\u001b[0m     beta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mdot(X2\u001b[39m.\u001b[39mT[rows], X2[:, cols]))\n\u001b[0;32m--> 315\u001b[0m     delta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49mdot(X\u001b[39m.\u001b[39;49mT[rows], X[:, cols]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m    316\u001b[0m rows \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(block_size \u001b[39m*\u001b[39m i, block_size \u001b[39m*\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    317\u001b[0m beta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mdot(X2\u001b[39m.\u001b[39mT[rows], X2[:, block_size \u001b[39m*\u001b[39m n_splits :]))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for atlas_path in atlases: ##loop over atlases\n",
    "    atlas_name = (atlas_path.split('/')[-1].split('.')[0].split('_')[0]) #split the atlas path so we\n",
    "    print(atlas_name)\n",
    "    al_p = (any(n == atlas_name for n in anatomical_label_names)) ##find wheter we have the anatomical labellings for this atlas\n",
    "\n",
    "    if al_p:\n",
    "        anatomic_path = path + f'/atlases/lawrance2021/label/Human/Anatomical-labels-csv/' + atlas + '.csv'\n",
    "        ana_labels = pd.read_csv(anatomic_path,names=colnames, header=None)\n",
    "    else:\n",
    "        ana_labels = None\n",
    "\n",
    "\n",
    "    time_series = connectome.calculate_time_series(atlas_path = atlas_path,imgs_paths=imgs_paths)     ##get the time series of the from all subjects, using the atlas defined\n",
    "\n",
    "    correlation_matrices, _ =  connectome.connectome(time_series = time_series,correlation_measure='correlation') #get the connectivity matrices\n",
    "\n",
    "    df_ = connectome.save_connectomes_df(correlation_matrices,anatomical_label_presence = al_p, anatomic_labels = ana_labels,path_to_save = path + 'results/', atlas_name = atlas_name, n_subjects = correlation_matrices.shape[0], correlation_measure = correlation_measure,subject_ixds = subjects_idxs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the regressons using the features extracted earlier\n",
    "Now that we have extracted the features of our fMRI data, we are ready to apply the regression models to calcualte the predictability of our target variables. We subdivide this task into three steps:\n",
    "1. loading and formatting our connectivity data\n",
    "2. loading and formattin our target variables\n",
    "3. fitting the desired model on the defined x and y\n",
    "\n",
    "This process needs to be repeated over all the atlases and other variables we have been looping over. This allows us to asses the question which steps of the pipline architecture have the largest effect on the prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sub-881_correlationType-correlation_atlas-Schaefer1000.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##let's find the paths to the the csv connectivity data we ahve\n",
    "data_paths = glob.glob(path + '/results/connectomes/*.csv')\n",
    "regressors_df =  regresson.load_regressors(path + 'func_images/AOMIC/regressors/*.txt')\n",
    "GCA = regressors_df.regressor_iq.values\n",
    "bmi = regressors_df.regressor_bmi.values\n",
    "##lets define pur target variable\n",
    " #concatenate cognitive metrics into single variable\n",
    "cognition = ['GCA','bmi']\n",
    "#cognition = ['PMAT Correct', 'PMAT Response Time']\n",
    "cog_metric = np.transpose(np.asarray([GCA, bmi]))\n",
    "\n",
    "for data_path in data_paths: ##loop over atlases\n",
    "\n",
    "    print(data_path.split('/')[-1])\n",
    "\n",
    "\n",
    "    loaded_data = regresson.load_data(data_path)\n",
    "\n",
    "    #concatenate cognitive metrics into single variable\n",
    "    cognition = ['GCA','bmi']\n",
    "    #cognition = ['PMAT Correct', 'PMAT Response Time']\n",
    "    cog_metric = np.transpose(np.asarray([GCA, bmi]))\n",
    "    #cog_metric = np.transpose(np.asarray([pmat_cr, pmat_med]))\n",
    "    cog_metric\n",
    "\n",
    "\n",
    "\n",
    "    #set the number of permutations you want to perform\n",
    "    perm = 5\n",
    "    #set the number of cross-validation loops you want to perform\n",
    "    cv_loops = 5\n",
    "    #set the number of folds you want in the inner and outer folds of the nested cross-validation\n",
    "    k = 3\n",
    "    #set the proportion of data you want in your training set\n",
    "    train_size = .8\n",
    "    #set the number of variable you want to predict to be the number of variables stored in the cognition variablse\n",
    "    n_cog = np.size(cognition)\n",
    "\n",
    "    #set regression model type\n",
    "\n",
    "    regr = Ridge(fit_intercept = True, max_iter=1000000)\n",
    "\n",
    "    #set hyperparameter grid space you want to search through for the model\n",
    "    alphas = np.linspace(100, 1000, num=10, endpoint=True, dtype=None, axis=0)\n",
    "\n",
    "    #set the param grid be to the hyperparamters you want to search through\n",
    "    paramGrid ={'alpha': alphas}\n",
    "\n",
    "    #set x data to be the input variable you want to use\n",
    "    #ie fc, sc, or hc\n",
    "    X = fc\n",
    "    X[X<0] = 0\n",
    "\n",
    "\n",
    "    # standardization -- optional (test if works better)\n",
    "    #X_norm = object.fit_transform(X) \n",
    "    #print(scale)\n",
    "\n",
    "    #set y to be the cognitive metrics you want to predict\n",
    "    Y = cog_metric\n",
    "    #Y - Y.mean(axis=0)\n",
    "\n",
    "    #set the number of features \n",
    "    n_feat = X.shape[1]\n",
    "\n",
    "\n",
    "    #create arrays to store variables\n",
    "    #r^2 - coefficient of determination\n",
    "    r2 = np.zeros([perm,n_cog])\n",
    "    #explained variance\n",
    "    var = np.zeros([perm,n_cog])\n",
    "    #correlation between true and predicted (aka prediction accuracy)\n",
    "    corr = np.zeros([perm,n_cog])\n",
    "    #optimised alpha (hyperparameter)\n",
    "    opt_alpha = np.zeros([perm,n_cog])\n",
    "    #predictions made by the model\n",
    "    preds = np.zeros([perm,n_cog,int(np.ceil(X.shape[0]*(1-train_size)))])\n",
    "    #true test values for cognition\n",
    "    cogtest = np.zeros([perm,n_cog,int(np.ceil(X.shape[0]*(1-train_size)))])\n",
    "    #feature importance extracted from the model\n",
    "    featimp = np.zeros([perm,n_feat,n_cog])\n",
    "\n",
    "\n",
    "\n",
    "        #iterate through permutations\n",
    "    for p in range(perm):\n",
    "        #print permutation # you're on\n",
    "        print('Permutation %d' %(p+1))\n",
    "        #split data into train and test sets\n",
    "        x_train, x_test, cog_train, cog_test = train_test_split(X, Y, test_size=1-train_size, \n",
    "                                                                shuffle=True, random_state=p)\n",
    "\n",
    "        \n",
    "        #iterate through the cognitive metrics you want to predict\n",
    "        for cog in range (n_cog):\n",
    "\n",
    "            #print cognitive metrics being predicted \n",
    "            print (\"Cognition: %s\" % cognition[cog])\n",
    "            \n",
    "            #set y values for train and test based on     \n",
    "            y_train = cog_train[:,cog]\n",
    "            y_test = cog_test[:,cog]\n",
    "            \n",
    "            #store all the y_test values in a separate variable that can be accessed later if needed\n",
    "            cogtest[p,cog,:] = y_test\n",
    "\n",
    "\n",
    "            #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "            nested_scores = []\n",
    "            best_params = []\n",
    "            \n",
    "\n",
    "            #optimise regression model using nested CV\n",
    "            print('Training Models')\n",
    "            \n",
    "            #go through the loops of the cross validation\n",
    "            for i in range(cv_loops):\n",
    "\n",
    "\n",
    "                #set parameters for inner and outer loops for CV\n",
    "                inner_cv = KFold(n_splits=k, shuffle=True, random_state=i)\n",
    "                outer_cv = KFold(n_splits=k, shuffle=True, random_state=i)\n",
    "                \n",
    "                #define regressor with grid-search CV for inner loop\n",
    "                gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, \n",
    "                                        verbose=0, cv=inner_cv, scoring='r2')\n",
    "\n",
    "                #fit regressor\n",
    "                gridSearch.fit(x_train, y_train)\n",
    "\n",
    "                #save parameters corresponding to the best score\n",
    "                best_params.append(list(gridSearch.best_params_.values()))\n",
    "\n",
    "                #call cross_val_score for outer loop\n",
    "                nested_score = cross_val_score(gridSearch, X=x_train, y=y_train, cv=outer_cv, \n",
    "                                            scoring='r2', verbose=1)\n",
    "\n",
    "                #record nested CV scores\n",
    "                nested_scores.append(np.median(nested_score))\n",
    "\n",
    "                #print how many cv loops are complete\n",
    "                print(\"%d/%d Complete\" % (i+1,cv_loops))\n",
    "                \n",
    "            #once all CV loops are complete, fit models based on optimised hyperparameters    \n",
    "            print('Testing Models')\n",
    "\n",
    "\n",
    "            #save optimised alpha values\n",
    "            opt_alpha[p,cog] = np.median(best_params)\n",
    "\n",
    "\n",
    "            #fit model using optimised hyperparameter\n",
    "            model = Ridge(fit_intercept = True, alpha = opt_alpha[p,cog], max_iter=1000000)\n",
    "            model.fit(x_train, y_train);\n",
    "            \n",
    "            #compute r^2 (coefficient of determination) \n",
    "            r2[p,cog]=model.score(x_test,y_test)\n",
    "\n",
    "            #generate predictions from model\n",
    "            preds[p,cog,:] = model.predict(x_test).ravel()\n",
    "            \n",
    "            #compute explained variance \n",
    "            var[p,cog] = explained_variance_score(y_test, preds[p,cog,:])\n",
    "\n",
    "            #compute correlation between true and predicted\n",
    "            corr[p,cog] = np.corrcoef(y_test, preds[p,cog,:])[1,0]\n",
    "\n",
    "            #extract feature importance\n",
    "            featimp[p,:,cog] = model.coef_\n",
    "\n",
    "    path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'r2.txt'\n",
    "\n",
    "    #save all of the outputs once everything is done\n",
    "    #filepath is where you want to store it\n",
    "\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'r2.txt', r2, delimiter=',')\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'var.txt', var, delimiter=',')\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'corr.txt', corr, delimiter=',')\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'opt_alpha.txt', opt_alpha, delimiter=',')\n",
    "\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'iq_ip.txt', featimp[:,:,0], delimiter=',')\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'bmi_ip.txt', featimp[:,:,1], delimiter=',')\n",
    "\n",
    "\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'iq_pres.txt', preds[:,0,:], delimiter=',')\n",
    "    np.savetxt(path + 'results/regression/' + data_path.split('/')[-1].split('.')[0] + 'bmi_preds.txt', preds[:,1,:], delimiter=',')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regressor_neo_n</th>\n",
       "      <th>regressor_ses</th>\n",
       "      <th>regressor_crystal</th>\n",
       "      <th>regressor_neo_o</th>\n",
       "      <th>regressor_fluid</th>\n",
       "      <th>regressor_neo_e</th>\n",
       "      <th>regressor_neo_c</th>\n",
       "      <th>regressor_age</th>\n",
       "      <th>regressor_iq</th>\n",
       "      <th>regressor_bmi</th>\n",
       "      <th>regressor_neo_a</th>\n",
       "      <th>missing</th>\n",
       "      <th>regressor_edu</th>\n",
       "      <th>subj</th>\n",
       "      <th>regressor_memory</th>\n",
       "      <th>regressor_bas_reward</th>\n",
       "      <th>subj_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>50</td>\n",
       "      <td>77.0</td>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>22.00</td>\n",
       "      <td>159.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>39</td>\n",
       "      <td>sub-0009</td>\n",
       "      <td>0</td>\n",
       "      <td>proc sub-0001</td>\n",
       "      <td>49.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>5.5</td>\n",
       "      <td>39.0</td>\n",
       "      <td>47</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>21.75</td>\n",
       "      <td>199.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>31</td>\n",
       "      <td>sub-0019</td>\n",
       "      <td>0</td>\n",
       "      <td>proc sub-0002</td>\n",
       "      <td>63.0</td>\n",
       "      <td>19</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>41</td>\n",
       "      <td>122.0</td>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>25.25</td>\n",
       "      <td>227.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>45</td>\n",
       "      <td>sub-0020</td>\n",
       "      <td>1</td>\n",
       "      <td>proc sub-0003</td>\n",
       "      <td>67.0</td>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>33</td>\n",
       "      <td>149.0</td>\n",
       "      <td>36</td>\n",
       "      <td>44</td>\n",
       "      <td>22.50</td>\n",
       "      <td>270.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36</td>\n",
       "      <td>sub-0023</td>\n",
       "      <td>1</td>\n",
       "      <td>proc sub-0004</td>\n",
       "      <td>69.0</td>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>4.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>35</td>\n",
       "      <td>112.0</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>22.25</td>\n",
       "      <td>212.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>40</td>\n",
       "      <td>sub-0026</td>\n",
       "      <td>1</td>\n",
       "      <td>proc sub-0005</td>\n",
       "      <td>57.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   regressor_neo_n  regressor_ses  regressor_crystal  regressor_neo_o   \n",
       "0               28            2.0               33.0               50  \\\n",
       "1               30            5.5               39.0               47   \n",
       "2               34            3.0               38.0               41   \n",
       "3               29            5.0               52.0               33   \n",
       "4               27            4.5               43.0               35   \n",
       "\n",
       "   regressor_fluid  regressor_neo_e  regressor_neo_c  regressor_age   \n",
       "0             77.0               50               47          22.00  \\\n",
       "1             97.0               46               45          21.75   \n",
       "2            122.0               40               29          25.25   \n",
       "3            149.0               36               44          22.50   \n",
       "4            112.0               44               44          22.25   \n",
       "\n",
       "   regressor_iq  regressor_bmi  regressor_neo_a   missing  regressor_edu   \n",
       "0         159.0           23.0               39  sub-0009              0  \\\n",
       "1         199.0           20.0               31  sub-0019              0   \n",
       "2         227.0           31.0               45  sub-0020              1   \n",
       "3         270.0           20.0               36  sub-0023              1   \n",
       "4         212.0           23.0               40  sub-0026              1   \n",
       "\n",
       "            subj  regressor_memory  regressor_bas_reward  subj_ids  \n",
       "0  proc sub-0001              49.0                    16       1.0  \n",
       "1  proc sub-0002              63.0                    19       2.0  \n",
       "2  proc sub-0003              67.0                    14       3.0  \n",
       "3  proc sub-0004              69.0                    18       4.0  \n",
       "4  proc sub-0005              57.0                    17       5.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
