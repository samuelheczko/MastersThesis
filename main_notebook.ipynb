{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we run thought the data aquisitin pipeline to get derive results discussed in Samuel Heczko's masters thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91172/1430019377.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import connectome\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import regresson\n",
    "\n",
    "import sys; sys.path\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "import imp\n",
    "\n",
    "\n",
    "path = 'data/' ##cahnge for cluster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the connectomes from preregistered fmri images\n",
    "In the thesis we largely conducted analysis on thethe Amsterdam Open MRI Collection (AOMIC). We used special preprocessed files and the data proveided by the AOIMIC people. We derive thea analysis in __ steps. All steps are done to all 900 subjects. We loop following proceedure over all atalases considererd.\n",
    "1. Calcluate the time series data for each brain region for each atlas (extract_connectomes.py)\n",
    "2. Compute the connectome of for each subject\n",
    "3. For each atlas save the upper triangle of the connectivity matrix along the information which brain regions are connected in this connection.\n",
    "\n",
    "\n",
    "Let's start with setting up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defime global parameters\n",
    "cluster = False\n",
    "correlation_measure = 'correlation' #can be also tangent, partial\n",
    "res_n = 1\n",
    "res = f'{res_n}x{res_n}x{res_n}'\n",
    "correlation_measure='correlation' ##set for calculation of the brain connectome, choose from 'correlation', 'tangent', 'partial' as implemented by nilearn\n",
    "#templateICBM = '/kyb/agks/sheczko/Downloads/MastersThesis/code/data/templates/mni_icbm152_nlin_asym_09c_nifti/mni_icbm152_nlin_asym_09c/mni_icbm152_t1_tal_nlin_asym_09c.nii' ##use the ICBM T1 template\n",
    "\n",
    "\n",
    "##add the data and names of thigs\n",
    "if cluster:\n",
    "    imgs_paths = glob.glob(path + 'func_images/AOMIC/prep_nifti/*.nii') ##load up a subset of the subejct images\n",
    "\n",
    "else:\n",
    "    imgs_paths = glob.glob(path + 'func_images/AOMIC/prep_nifti/*000*.nii') ##load up all the subejct images\n",
    "subjects_idxs = []\n",
    "for s_n in imgs_paths:\n",
    "    subjects_idxs.append(s_n.split('_')[-1].split('.')[0]) #split the path to extract only the number of the subject from path\n",
    "\n",
    "\n",
    "\n",
    "atlases = glob.glob(path + '/atlases/lawrance2021/label/Human/ICBM/*.nii.gz') ##get the atlases\n",
    "anatomical_labels = glob.glob(path + '/atlases/lawrance2021/label/Human/Anatomical-labels-csv/*.csv') #get the anatomical labels (where available)\n",
    "anatomical_label_names = []\n",
    "for a_l in anatomical_labels:\n",
    "    anatomical_label_names.append(a_l.split('/')[-1].split('.')[0]) #split the path to extract only the name of the atlas\n",
    "\n",
    "\n",
    "##loop over atlases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiune with looping over the list of atlases and saving a csv wiht connectivity for all subejcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS00108\n",
      "DS06481\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     ana_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m time_series \u001b[39m=\u001b[39m connectome\u001b[39m.\u001b[39mcalculate_time_series(atlas_path \u001b[39m=\u001b[39m atlas_path,imgs_paths\u001b[39m=\u001b[39mimgs_paths)     \u001b[39m##get the time series of the from all subjects, using the atlas defined\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m correlation_matrices, _ \u001b[39m=\u001b[39m  connectome\u001b[39m.\u001b[39;49mconnectome(time_series \u001b[39m=\u001b[39;49m time_series,correlation_measure\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcorrelation\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m#get the connectivity matrices\u001b[39;00m\n\u001b[1;32m     17\u001b[0m df_ \u001b[39m=\u001b[39m connectome\u001b[39m.\u001b[39msave_connectomes_df(correlation_matrices,anatomical_label_presence \u001b[39m=\u001b[39m al_p, anatomic_labels \u001b[39m=\u001b[39m ana_labels,path_to_save \u001b[39m=\u001b[39m path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mresults/\u001b[39m\u001b[39m'\u001b[39m, atlas_name \u001b[39m=\u001b[39m atlas_name, n_subjects \u001b[39m=\u001b[39m correlation_matrices\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], correlation_measure \u001b[39m=\u001b[39m correlation_measure,subject_ixds \u001b[39m=\u001b[39m subjects_idxs)\n",
      "File \u001b[0;32m~/Downloads/MastersThesis/code/connectome.py:29\u001b[0m, in \u001b[0;36mconnectome\u001b[0;34m(time_series, correlation_measure)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnectome\u001b[39m(time_series,correlation_measure \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[39m##INPUT: the time series on each brain region as list(?) of arrays (for each participant) given by nilearn, the desired correlation type (choose from linear correaltin, partial correlation, tangent correlation)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m##OUTPUT: signal (standartised)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     connectome_measure \u001b[39m=\u001b[39m ConnectivityMeasure(kind \u001b[39m=\u001b[39m correlation_measure)\n\u001b[0;32m---> 29\u001b[0m     correlation_matrices \u001b[39m=\u001b[39m connectome_measure\u001b[39m.\u001b[39;49mfit_transform(time_series) \n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m correlation_matrices,connectome_measure\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:569\u001b[0m, in \u001b[0;36mConnectivityMeasure.fit_transform\u001b[0;34m(self, X, y, confounds)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    565\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTangent space parametrization can only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbe applied to a group of subjects, as it returns \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdeviations to the mean. You provided \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m X\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, do_fit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, do_transform\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    570\u001b[0m                            confounds\u001b[39m=\u001b[39;49mconfounds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36mConnectivityMeasure._fit_transform\u001b[0;34m(self, X, do_transform, do_fit, confounds)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcov_estimator_\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39m_standardize(x, detrend\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, standardize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nilearn/connectome/connectivity_matrices.py:473\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m# Compute all the matrices, stored in \"connectivities\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcorrelation\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 473\u001b[0m     covariances_std \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcov_estimator_\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    474\u001b[0m         signal\u001b[39m.\u001b[39;49m_standardize(x, detrend\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, standardize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    475\u001b[0m         )\u001b[39m.\u001b[39mcovariance_ \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m    476\u001b[0m     connectivities \u001b[39m=\u001b[39m [cov_to_corr(cov) \u001b[39mfor\u001b[39;00m cov \u001b[39min\u001b[39;00m covariances_std]\n\u001b[1;32m    477\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:545\u001b[0m, in \u001b[0;36mLedoitWolf.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m    544\u001b[0m \u001b[39mwith\u001b[39;00m config_context(assume_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 545\u001b[0m     covariance, shrinkage \u001b[39m=\u001b[39m ledoit_wolf(\n\u001b[1;32m    546\u001b[0m         X \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocation_, assume_centered\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, block_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrinkage_ \u001b[39m=\u001b[39m shrinkage\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_covariance(covariance)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:397\u001b[0m, in \u001b[0;36mledoit_wolf\u001b[0;34m(X, assume_centered, block_size)\u001b[0m\n\u001b[1;32m    394\u001b[0m     _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[1;32m    396\u001b[0m \u001b[39m# get Ledoit-Wolf shrinkage\u001b[39;00m\n\u001b[0;32m--> 397\u001b[0m shrinkage \u001b[39m=\u001b[39m ledoit_wolf_shrinkage(\n\u001b[1;32m    398\u001b[0m     X, assume_centered\u001b[39m=\u001b[39;49massume_centered, block_size\u001b[39m=\u001b[39;49mblock_size\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m emp_cov \u001b[39m=\u001b[39m empirical_covariance(X, assume_centered\u001b[39m=\u001b[39massume_centered)\n\u001b[1;32m    401\u001b[0m mu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mtrace(emp_cov)) \u001b[39m/\u001b[39m n_features\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/covariance/_shrunk_covariance.py:315\u001b[0m, in \u001b[0;36mledoit_wolf_shrinkage\u001b[0;34m(X, assume_centered, block_size)\u001b[0m\n\u001b[1;32m    313\u001b[0m     cols \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(block_size \u001b[39m*\u001b[39m j, block_size \u001b[39m*\u001b[39m (j \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    314\u001b[0m     beta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mdot(X2\u001b[39m.\u001b[39mT[rows], X2[:, cols]))\n\u001b[0;32m--> 315\u001b[0m     delta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49mdot(X\u001b[39m.\u001b[39;49mT[rows], X[:, cols]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m    316\u001b[0m rows \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(block_size \u001b[39m*\u001b[39m i, block_size \u001b[39m*\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    317\u001b[0m beta_ \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mdot(X2\u001b[39m.\u001b[39mT[rows], X2[:, block_size \u001b[39m*\u001b[39m n_splits :]))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for atlas_path in atlases: ##loop over atlases\n",
    "    atlas_name = (atlas_path.split('/')[-1].split('.')[0].split('_')[0]) #split the atlas path so we\n",
    "    print(atlas_name)\n",
    "    al_p = (any(n == atlas_name for n in anatomical_label_names)) ##find wheter we have the anatomical labellings for this atlas\n",
    "\n",
    "    if al_p:\n",
    "        anatomic_path = path + f'/atlases/lawrance2021/label/Human/Anatomical-labels-csv/' + atlas + '.csv'\n",
    "        ana_labels = pd.read_csv(anatomic_path,names=colnames, header=None)\n",
    "    else:\n",
    "        ana_labels = None\n",
    "\n",
    "\n",
    "    time_series = connectome.calculate_time_series(atlas_path = atlas_path,imgs_paths=imgs_paths)     ##get the time series of the from all subjects, using the atlas defined\n",
    "\n",
    "    correlation_matrices, _ =  connectome.connectome(time_series = time_series,correlation_measure='correlation') #get the connectivity matrices\n",
    "\n",
    "    df_ = connectome.save_connectomes_df(correlation_matrices,anatomical_label_presence = al_p, anatomic_labels = ana_labels,path_to_save = path + 'results/', atlas_name = atlas_name, n_subjects = correlation_matrices.shape[0], correlation_measure = correlation_measure,subject_ixds = subjects_idxs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the regressons using the features extracted earlier\n",
    "This part of the code is also regression_launcher.py\n",
    "Now that we have extracted the features of our fMRI data, we are ready to apply the regression models to calcualte the predictability of our target variables. We subdivide this task into three steps:\n",
    "1. loading and formatting our connectivity data\n",
    "2. loading and formattin our target variables\n",
    "3. fitting the desired model on the defined x and y\n",
    "\n",
    "This process needs to be repeated over all the atlases and other variables we have been looping over. This allows us to asses the question which steps of the pipline architecture have the largest effect on the prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current atlas: AAL\n",
      "Permutation 1\n",
      "Cognition: GCA\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Cognition: bmi\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Permutation 2\n",
      "Cognition: GCA\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Cognition: bmi\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Permutation 3\n",
      "Cognition: GCA\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Cognition: bmi\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Permutation 4\n",
      "Cognition: GCA\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Cognition: bmi\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Permutation 5\n",
      "Cognition: GCA\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n",
      "Cognition: bmi\n",
      "Training Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 Complete\n",
      "Testing Models\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m result_df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcog_names\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_var\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m var[:,cog_i]\n\u001b[1;32m     60\u001b[0m result_df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcog_names\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_opt_alphas\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m opt_alphas[:,cog_i]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mfor\u001b[39;00m perm_n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(reg\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     62\u001b[0m     preds_df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcog_names\u001b[39m}\u001b[39;00m\u001b[39m_perm_\u001b[39m\u001b[39m{\u001b[39;00mperm_n\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_preds\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m preds[perm_n,cog_i]\n\u001b[1;32m     63\u001b[0m     preds_df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcog_names\u001b[39m}\u001b[39;00m\u001b[39m_perm\u001b[39m\u001b[39m{\u001b[39;00mperm_n\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,_real\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cogtest[perm_n,n,cog_i,:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "imp.reload(regresson)\n",
    "\n",
    "\n",
    "##let's find the paths to the the csv connectivity data we ahve\n",
    "data_paths = glob.glob(path + '/results/connectomes/*.csv')\n",
    "\n",
    "##load up regresors\n",
    "regressors_df =  regresson.load_regressors(path + 'func_images/AOMIC/regressors/*.txt')\n",
    "##choose the target variables, take as np arrays\n",
    "GCA = regressors_df.regressor_iq.values\n",
    "bmi = regressors_df.regressor_bmi.values\n",
    "\n",
    " #concatenate cognitive metrics into single variable\n",
    "cognition = ['GCA','bmi']\n",
    "#cognition = ['PMAT Correct', 'PMAT Response Time']\n",
    "cog_metric = np.transpose(np.asarray([GCA, bmi]))\n",
    "\n",
    "#set the number of permutations you want to perform\n",
    "perm = 5\n",
    "    #set the number of cross-validation loops you want to perform\n",
    "cv_loops = 5\n",
    "#set the number of folds you want in the inner and outer folds of the nested cross-validation\n",
    "k = 3\n",
    "#set the proportion of data you want in your training set\n",
    "train_size = .8\n",
    "#set the number of variable you want to predict to be the number of variables stored in the cognition variablse\n",
    "n_cog = np.size(cognition)\n",
    "#set regression model type\n",
    "regr = Ridge(fit_intercept = True, max_iter=1000000)\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "alphas = np.linspace(100, 1000, num=10, endpoint=True, dtype=None, axis=0)\n",
    "#set y to be the cognitive metrics you want to predict. They are the same for every atlas (each subject has behavioural score regradless of parcellation)\n",
    "Y = cog_metric\n",
    "\n",
    "\n",
    "for data_path_i, data_path in enumerate(data_paths): ##loop over atlases\n",
    "    \n",
    "    current_atlas = data_paths[data_path_i+1].split('/')[-1].split('-')[-1].split('.')[0]\n",
    "    print(f'current atlas: ' + current_atlas)\n",
    "\n",
    "    fc = regresson.load_data(data_paths[data_path_i+1]) ##set the imput variable to the current atlas connectome, gives subjects x features matrix\n",
    "\n",
    "    #set x data to be the input variable you want to use\n",
    "    #ie fc, sc, or hc\n",
    "    #cut the negative values from the correlations\n",
    "    X = fc\n",
    "    X[X<0] = 0\n",
    "\n",
    "\n",
    "    #set the number of features \n",
    "    n_feat = X.shape[1]\n",
    "    r2, preds, var, corr, featimp, cogtest,opt_alphas = regresson.regression(X = X, Y = Y, perm = perm, cv_loops = cv_loops, k = k, train_size = 0.8, n_cog = n_cog, regr = regr, alphas = alphas,n_feat = n_feat,cognition = cognition)\n",
    "    \n",
    "    ##save data:\n",
    "    result_df = pd.DataFrame() ## empty dataframe, filled with colums where each row is one permutation\n",
    "    preds_df = pd.DataFrame()\n",
    "\n",
    "    for cog_i, cog_names in enumerate(cognition):\n",
    "            result_df[f'{cog_names}' + '_r2'] = r2[:,cog_i] #fill in the r2 values\n",
    "            result_df[f'{cog_names}' + '_var'] = var[:,cog_i] #fill in explained variation values\n",
    "            result_df[f'{cog_names}' + '_opt_alphas'] = opt_alphas[:,cog_i] #fill in optimal alpha values\n",
    "            for perm_n in range(preds.shape[0]): \n",
    "                preds_df[f'{cog_names}_perm_{perm_n + 1}_preds'] = preds[perm_n,cog_i,:] #fill in preditions\n",
    "                preds_df[f'{cog_names}_perm_{perm_n + 1}_real'] = cogtest[perm_n,cog_i,:] #fill in real_values\n",
    "    result_df.to_csv(path + f'results/ridge_regression/ridge_results_atlas-{current_atlas}.csv')\n",
    "    preds_df.to_csv(path + f'results/ridge_regression/ridge_preds_atlas-{current_atlas}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save data:\n",
    "result_df = pd.DataFrame() ## empty dataframe, filled with colums where each row is one permutation\n",
    "preds_df = pd.DataFrame()\n",
    "for cog_i, cog_names in enumerate(cognition):\n",
    "        result_df[f'{cog_names}' + '_r2'] = r2[:,cog_i]\n",
    "        result_df[f'{cog_names}' + '_var'] = var[:,cog_i]\n",
    "        result_df[f'{cog_names}' + '_opt_alphas'] = opt_alphas[:,cog_i]\n",
    "        for perm_n in range(preds.shape[0]):\n",
    "            preds_df[f'{cog_names}_perm_{perm_n + 1}_preds'] = preds[perm_n,cog_i,:]\n",
    "            preds_df[f'{cog_names}_perm_{perm_n + 1}_real'] = cogtest[perm_n,cog_i,:]\n",
    "result_df.to_csv(path + f'results/ridge_regression/ridge_results_atlas-{current_atlas}.csv')\n",
    "preds_df.to_csv(path + f'results/ridge_regression/ridge_preds_atlas-{current_atlas}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "for cog_i, cog_names in enumerate(cognition):\n",
    "        result_df[f'{cog_names}' + '_r2'] = r2[:,cog_i]\n",
    "        result_df[f'{cog_names}' + '_var'] = var[:,cog_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = np.random.random((5,2))\n",
    "var = np.random.random((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35293049 0.9468528 ]\n",
      " [0.83402726 0.93571856]\n",
      " [0.2929712  0.62324246]\n",
      " [0.49220652 0.19135977]\n",
      " [0.44216661 0.06799257]]\n",
      "[[0.1901317  0.42403592]\n",
      " [0.23709415 0.02885403]\n",
      " [0.42005898 0.70998366]\n",
      " [0.55858748 0.19009908]\n",
      " [0.56388484 0.82895028]]\n"
     ]
    }
   ],
   "source": [
    "print(r2)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCA_r2</th>\n",
       "      <th>GCA_var</th>\n",
       "      <th>bmi_r2</th>\n",
       "      <th>bmi_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352930</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>0.946853</td>\n",
       "      <td>0.424036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.237094</td>\n",
       "      <td>0.935719</td>\n",
       "      <td>0.028854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.292971</td>\n",
       "      <td>0.420059</td>\n",
       "      <td>0.623242</td>\n",
       "      <td>0.709984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.492207</td>\n",
       "      <td>0.558587</td>\n",
       "      <td>0.191360</td>\n",
       "      <td>0.190099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442167</td>\n",
       "      <td>0.563885</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>0.828950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GCA_r2   GCA_var    bmi_r2   bmi_var\n",
       "0  0.352930  0.190132  0.946853  0.424036\n",
       "1  0.834027  0.237094  0.935719  0.028854\n",
       "2  0.292971  0.420059  0.623242  0.709984\n",
       "3  0.492207  0.558587  0.191360  0.190099\n",
       "4  0.442167  0.563885  0.067993  0.828950"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
