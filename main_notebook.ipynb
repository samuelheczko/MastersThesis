{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we run thought the data acquisition pipeline to get derive results discussed in paper ....\n",
    "\n",
    "In this exmaple we consider looping over teh altalse included in the neruoparc dataset. The atlases used are defined part 1 of the pipeline, where we define the variable 'atlases'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836905/2957888314.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "#mports\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import connectome\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import regresson\n",
    "\n",
    "import sys; sys.path\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "import imp\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "\n",
    "path = '/kyb/agks/sheczko/Downloads/MastersThesis/software/Ridge-Regression-Predictions/data/' ##define the location of the data file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the connectomes from preregistered fmri images\n",
    "In the thesis we largely conducted analysis on thethe Amsterdam Open MRI Collection (AOMIC). We used special preprocessed files and the data proveided by the AOIMIC people. We derive thea analysis in __ steps. All steps are done to all 900 subjects. We loop following proceedure over all atalases considererd.\n",
    "1. Calcluate the time series data for each brain region for each atlas (extract_connectomes.py)\n",
    "2. Compute the connectome of for each subject\n",
    "3. For each atlas save the upper triangle of the connectivity matrix along the information which brain regions are connected in this connection.\n",
    "\n",
    "\n",
    "Let's start with setting up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defime global parameters\n",
    "cluster = False\n",
    "correlation_measure = 'correlation' #can be also tangent, partial\n",
    "res_n = 1\n",
    "res = f'{res_n}x{res_n}x{res_n}'\n",
    "correlation_measure='correlation' ##set for calculation of the brain connectome, choose from 'correlation', 'tangent', 'partial' as implemented by nilearn\n",
    "#templateICBM = '/kyb/agks/sheczko/Downloads/MastersThesis/code/data/templates/mni_icbm152_nlin_asym_09c_nifti/mni_icbm152_nlin_asym_09c/mni_icbm152_t1_tal_nlin_asym_09c.nii' ##use the ICBM T1 template\n",
    "\n",
    "\n",
    "##add the data and names of thigs\n",
    "imgs_paths = glob.glob(path + 'func_images/AOMIC/prep_nifti/*000*.nii') ##for an example - load up a subset of the subejct images\n",
    "\n",
    "subjects_idxs = []\n",
    "for s_n in imgs_paths:\n",
    "    subjects_idxs.append(s_n.split('_')[-1].split('.')[0]) #split the path to extract only the number of the subject from path\n",
    "\n",
    "\n",
    "\n",
    "atlases = glob.glob(path + '/atlases/lawrance2021/label/Human/ICBM/*AAL*.nii.gz') ##load up the AAL atlas for example\n",
    "anatomical_labels = glob.glob(path + '/atlases/lawrance2021/label/Human/Anatomical-labels-csv/*.csv') #get the anatomical labels (where available)\n",
    "anatomical_label_names = []\n",
    "for a_l in anatomical_labels:\n",
    "    anatomical_label_names.append(a_l.split('/')[-1].split('.')[0]) #split the path to extract only the name of the atlas\n",
    "\n",
    "colnames=['idx','anatomical_label'] \n",
    "\n",
    "\n",
    "##loop over atlases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiune with looping over the list of atlases and saving a csv wiht connectivity for all subejcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 0.91s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 1.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 1.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 0.95s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 0.90s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/nilearn/maskers/nifti_labels_masker.py:617: UserWarning: Persisting input arguments took 0.74s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  region_signals, labels_ = self._cache(\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.26s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 0.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n",
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/joblib/parallel.py:288: UserWarning: Persisting input arguments took 0.84s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  return [func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 8 subjects and (290, 116) shaped time series\n"
     ]
    }
   ],
   "source": [
    "for atlas_path in atlases: ##loop over atlases\n",
    "    atlas_name = (atlas_path.split('/')[-1].split('.')[0].split('_')[0]) #split the atlas path so we\n",
    "    print(atlas_name)\n",
    "    al_p = (any(n == atlas_name for n in anatomical_label_names)) ##find wheter we have the anatomical labellings for this atlas\n",
    "\n",
    "    if al_p:\n",
    "        anatomic_path = path + f'/atlases/lawrance2021/label/Human/Anatomical-labels-csv/' + atlas_name + '.csv'\n",
    "        ana_labels = pd.read_csv(anatomic_path,names=colnames, header=None)\n",
    "        ana_labels = ana_labels[ana_labels['idx'] != 0]\n",
    "    else:\n",
    "        ana_labels = None\n",
    "\n",
    "\n",
    "    time_series = connectome.calculate_time_series(atlas_path = atlas_path,imgs_paths=imgs_paths)     ##get the time series of the from all subjects, using the atlas defined\n",
    "    print(f'we have {len(time_series)} subjects and {time_series[0].shape} shaped time series')\n",
    "    df_time_series = pd.DataFrame()\n",
    "    \n",
    "    ##save time series\n",
    "    for array_i, array in enumerate(time_series):\n",
    "        df_time_series = pd.concat([df_time_series,pd.DataFrame(data = array.flatten(), columns=['subj_' + str(array_i)] )], axis = 1)\n",
    "        \n",
    "    Brn_area_indecies = np.repeat(np.arange(time_series[0].shape[1]),time_series[0].shape[0])\n",
    "    if al_p:\n",
    "        df_labels = pd.DataFrame(data = ana_labels.iloc[Brn_area_indecies,1],columns=['anatomical_label'])\n",
    "        df_labels = df_labels.reset_index()\n",
    "        df_time_series = pd.concat([df_time_series,df_labels],axis = 1)\n",
    "    else:\n",
    "        df_time_series = pd.concat([df_time_series,pd.DataFrame(data = Brn_area_indecies,columns = ['brain_area_index'])],axis = 1)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    #df_time_series.to_csv(path_or_buf = path + f'/results/time_series/time_series_n_sub-{len(time_series)}_atlas-{atlas_name}.csv') ##save the time serie \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    correlation_matrices, _ =  connectome.connectome(time_series = time_series,correlation_measure='correlation') #get the connectivity matrices\n",
    "    ##save the connectome\n",
    "    df_ = connectome.save_connectomes_df(correlation_matrices,anatomical_label_presence = al_p, anatomic_labels = ana_labels,path_to_save = path + 'results/connectomes/examples/', atlas_name = atlas_name, n_subjects = correlation_matrices.shape[0], correlation_measure = correlation_measure,subject_ixds = subjects_idxs) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the regressons using the features extracted earlier\n",
    "This part of the code is also regression_launcher.py\n",
    "Now that we have extracted the features of our fMRI data, we are ready to apply the regression models to calcualte the predictability of our target variables. We subdivide this task into three steps:\n",
    "1. loading and formatting our connectivity data\n",
    "2. loading and formattin our target variables\n",
    "3. fitting the desired model on the defined x and y\n",
    "\n",
    "This process needs to be repeated over all the atlases and other variables we have been looping over. This allows us to asses the question which steps of the pipline architecture have the largest effect on the prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current atlas: atlas-DS00108\n",
      "data loaded:)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "regression() got an unexpected keyword argument 'fit_intercept'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[39m#set the number of features \u001b[39;00m\n\u001b[1;32m     73\u001b[0m     n_feat \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m     r2_iq_fMRI_preds, r2_iq_edu_preds, r2_iq_avg_preds, r2_iq_resid_preds, r2_preds_edu, corr_iq_fMRI_preds, corr_iq_edu_preds, corr_iq_avg_preds, corr_iq_resid_preds,corr_preds_edu, n_pred, cogtest, featimp,preds, preds2, preds3,var,opt_alpha \u001b[39m=\u001b[39m regresson\u001b[39m.\u001b[39;49mregression(X \u001b[39m=\u001b[39;49m X, Y \u001b[39m=\u001b[39;49m Y, perm \u001b[39m=\u001b[39;49m perm, cv_loops \u001b[39m=\u001b[39;49m cv_loops, k \u001b[39m=\u001b[39;49m k, train_size \u001b[39m=\u001b[39;49m train_size, n_cog \u001b[39m=\u001b[39;49m n_cog, regr \u001b[39m=\u001b[39;49m regr, alphas \u001b[39m=\u001b[39;49m alphas,n_feat \u001b[39m=\u001b[39;49m n_feat,\n\u001b[1;32m     77\u001b[0m         cognition \u001b[39m=\u001b[39;49m cognition,manual_folds \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, n_iter_search\u001b[39m=\u001b[39;49mn_iter,Feature_selection \u001b[39m=\u001b[39;49m Feature_selection,z_score \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,fit_intercept \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     80\u001b[0m         \u001b[39m##save data:\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m#1 make df of the predicted values \u001b[39;00m\n\u001b[1;32m     82\u001b[0m     df_preds \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(preds\u001b[39m.\u001b[39mreshape(perm \u001b[39m*\u001b[39m n_cog,n_pred)\u001b[39m.\u001b[39mT,columns \u001b[39m=\u001b[39m column_names_pred) \u001b[39m## we flatten the permutation axis \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: regression() got an unexpected keyword argument 'fit_intercept'"
     ]
    }
   ],
   "source": [
    "imp.reload(regresson)\n",
    "\n",
    "##let's find the paths to the the csv connectivity data we have\n",
    "data_paths = glob.glob(path + '/results/connectomes/examples/*.csv')\n",
    "\n",
    "##load up regresors\n",
    "regressors_df =  regresson.load_regressors(path + 'func_images/AOMIC/regressors/*.txt')\n",
    "##choose the target variables, take as np arrays\n",
    "GCA = regressors_df.regressor_iq.values\n",
    "bmi = regressors_df.regressor_bmi.values\n",
    "\n",
    " #concatenate cognitive metrics into single variable\n",
    "cognition = ['GCA','bmi']\n",
    "#cognition = ['PMAT Correct', 'PMAT Response Time']\n",
    "cog_metric = np.transpose(np.asarray([GCA, bmi]))\n",
    "\n",
    "#set the number of permutations you want to perform\n",
    "perm = 3\n",
    "#set the number of cross-validation loops you want to perform\n",
    "cv_loops = 5\n",
    "#set the number of folds you want in the inner and outer folds of the nested cross-validation\n",
    "k = 3\n",
    "#set the proportion of data you want in your training set\n",
    "train_size = .8\n",
    "#set the number of variable you want to predict to be the number of variables stored in the cognition variablse\n",
    "n_cog = np.size(cognition)\n",
    "#set regression model type\n",
    "regr = Ridge(fit_intercept = False, max_iter=1000000)\n",
    "alphas = loguniform(10, 10e3) #define distribution\n",
    "n_iter = 100 #amounf of random guesses\n",
    "\n",
    "#set y to be the cognitive metrics you want to predict. They are the same for every atlas (each subject has behavioural score regradless of parcellation)\n",
    "Y = cog_metric\n",
    "\n",
    "\n",
    "column_names_pred = []\n",
    "column_names_real = []\n",
    "\n",
    "\n",
    "Feature_selection = False ##set the whether to use the feature selection trick based on the education scores\n",
    "\n",
    "\n",
    "\n",
    "for perm_ixd in range(perm):\n",
    "    for cog in cognition:\n",
    "        column_names_pred.append(f'{cog}_perm_{perm_ixd + 1}_pred')\n",
    "        column_names_real.append(f'{cog}_perm_{perm_ixd + 1}_real')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data_path_i, data_path in enumerate(data_paths): ##loop over atlases\n",
    "    \n",
    "\n",
    "    current_path = data_paths[data_path_i + 1]\n",
    "    current_atlas = current_path.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    print(f'current atlas: ' + current_atlas)\n",
    "\n",
    "    fc = regresson.load_data(current_path) ##set the imput variable to the current atlas connectome, gives subjects x features matrix\n",
    "\n",
    "    print(f'data loaded:)')\n",
    "\n",
    "    #set x data to be the input variable you want to use\n",
    "    #ie fc, sc, or hc\n",
    "    #cut the negative values from the correlations\n",
    "    X = fc\n",
    "    X[X<0] = 0\n",
    "\n",
    "\n",
    "    #set the number of features \n",
    "    n_feat = X.shape[1]\n",
    "    \n",
    "\n",
    "    r2_iq_fMRI_preds, r2_iq_edu_preds, r2_iq_avg_preds, r2_iq_resid_preds, r2_preds_edu, corr_iq_fMRI_preds, corr_iq_edu_preds, corr_iq_avg_preds, corr_iq_resid_preds,corr_preds_edu, n_pred, cogtest, featimp,preds, preds2, preds3,var,opt_alpha = regresson.regression(X = X, Y = Y, perm = perm, cv_loops = cv_loops, k = k, train_size = train_size, n_cog = n_cog, regr = regr, alphas = alphas,n_feat = n_feat,\n",
    "        cognition = cognition,manual_folds = False, n_iter_search=n_iter,Feature_selection = Feature_selection,z_score = False,fit_intercept = True)\n",
    "\n",
    "        \n",
    "        ##save data:\n",
    "#1 make df of the predicted values \n",
    "    df_preds = pd.DataFrame(preds.reshape(perm * n_cog,n_pred).T,columns = column_names_pred) ## we flatten the permutation axis \n",
    "\n",
    "    df_real = pd.DataFrame(cogtest.reshape(perm * n_cog,n_pred).T,columns = column_names_real)\n",
    "    preds_real_df = pd.concat([df_preds,df_real],axis = 1, sort = True)\n",
    "\n",
    "#2 make df of the statistical values\n",
    "\n",
    "    result_r2 = pd.DataFrame(columns = [cog + '_r2' for cog in cognition], data = r2_iq_fMRI_preds)\n",
    "    result_r2_edu = pd.DataFrame(columns = [cog + '_r2_using_only_edu' for cog in cognition], data = r2_iq_edu_preds)\n",
    "    result_r2_2 = pd.DataFrame(columns = [cog + '_r2_averaged_FA' for cog in cognition], data = r2_iq_avg_preds)\n",
    "    result_r2_resid = pd.DataFrame(columns = [cog + '_r2_after_controlling_residuals' for cog in cognition], data = r2_iq_avg_preds)\n",
    "\n",
    "    result_r2_pred_edu = pd.DataFrame(columns = [cog + '_r2_to_edu_pred' for cog in cognition], data = r2_preds_edu)\n",
    "\n",
    "\n",
    "    result_corr = pd.DataFrame(columns = [cog + '_corr' for cog in cognition], data = corr_iq_fMRI_preds)\n",
    "    result_corr_edu = pd.DataFrame(columns = [cog + '_corr_using_only_edu' for cog in cognition], data = corr_iq_edu_preds)\n",
    "    result_corr_2 = pd.DataFrame(columns = [cog + '_corr_averaged_FA' for cog in cognition], data = corr_iq_avg_preds)\n",
    "    result_corr_resid = pd.DataFrame(columns = [cog + '_corr_after_controlling_residuals' for cog in cognition], data = corr_iq_resid_preds)\n",
    "    \n",
    "    result_corr_pred_edu = pd.DataFrame(columns = [cog + '_corr_edu_pred' for cog in cognition], data = corr_preds_edu)\n",
    "\n",
    "\n",
    "\n",
    "    result_var = pd.DataFrame(columns = [cog + '_var' for cog in cognition], data = var)\n",
    "    opt_alphas_df = pd.DataFrame(columns = [cog + '_opt_alphas' for cog in cognition], data =  opt_alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result_df = pd.concat([result_var,result_r2,result_r2_edu,result_r2_2,result_r2_resid,result_r2_pred_edu,result_corr,result_corr_edu,result_corr_2,result_corr_resid,result_corr_pred_edu,opt_alphas_df],axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    ##save data:\n",
    "\n",
    "    result_df.to_csv(path + f'results/regression_results/ridge_regression/examples/ridge_results_atlas-{current_atlas}.csv')\n",
    "    preds_real_df.to_csv(path + f'results/regression_results/ridge_regression/examples/ridge_preds_atlas-{current_atlas}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names_pred = []\n",
    "column_names_real = []    \n",
    "\n",
    "\n",
    "for perm_ixd in range(perm):\n",
    "    for cog in cognition:\n",
    "        column_names_pred.append(f'{cog}_perm_{perm_ixd + 1}_pred')\n",
    "        column_names_real.append(f'{cog}_perm_{perm_ixd + 1}_real')\n",
    "    \n",
    "    \n",
    "##save data:\n",
    "\n",
    "df_preds = pd.DataFrame(preds.reshape(perm * n_cog,n_pred).T,columns = column_names_pred) ## we flatten the permutation axis \n",
    "df_real = pd.DataFrame(cogtest.reshape(perm * n_cog,n_pred).T,columns = column_names_real)\n",
    "preds_real_df = pd.concat([df_preds,df_real],axis = 1, sort = True)\n",
    "\n",
    "\n",
    "result_r2 = pd.DataFrame(columns = [cog + '_r2' for cog in cognition], data = r2)\n",
    "result_var = pd.DataFrame(columns = [cog + '_var' for cog in cognition], data = var)\n",
    "opt_alphas_df = pd.DataFrame(columns = [cog + '_opt_alphas' for cog in cognition])\n",
    "result_df = pd.concat([result_var,result_r2],axis = 1)\n",
    "\n",
    "result_df.to_csv(path + f'results/ridge_regression/ridge_results_atlas-{current_atlas}.csv')\n",
    "preds_real_df.to_csv(path + f'results/ridge_regression/ridge_preds_atlas-{current_atlas}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCA_perm_1_pred</th>\n",
       "      <th>bmi_perm_1_pred</th>\n",
       "      <th>GCA_perm_2_pred</th>\n",
       "      <th>bmi_perm_2_pred</th>\n",
       "      <th>GCA_perm_3_pred</th>\n",
       "      <th>bmi_perm_3_pred</th>\n",
       "      <th>GCA_perm_4_pred</th>\n",
       "      <th>bmi_perm_4_pred</th>\n",
       "      <th>GCA_perm_5_pred</th>\n",
       "      <th>bmi_perm_5_pred</th>\n",
       "      <th>GCA_perm_1_real</th>\n",
       "      <th>bmi_perm_1_real</th>\n",
       "      <th>GCA_perm_2_real</th>\n",
       "      <th>bmi_perm_2_real</th>\n",
       "      <th>GCA_perm_3_real</th>\n",
       "      <th>bmi_perm_3_real</th>\n",
       "      <th>GCA_perm_4_real</th>\n",
       "      <th>bmi_perm_4_real</th>\n",
       "      <th>GCA_perm_5_real</th>\n",
       "      <th>bmi_perm_5_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186.071931</td>\n",
       "      <td>24.135502</td>\n",
       "      <td>209.310151</td>\n",
       "      <td>24.111114</td>\n",
       "      <td>204.221377</td>\n",
       "      <td>25.525347</td>\n",
       "      <td>200.576436</td>\n",
       "      <td>24.449453</td>\n",
       "      <td>197.981595</td>\n",
       "      <td>22.436691</td>\n",
       "      <td>130.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197.944629</td>\n",
       "      <td>24.089197</td>\n",
       "      <td>183.521658</td>\n",
       "      <td>24.904964</td>\n",
       "      <td>187.247643</td>\n",
       "      <td>23.118162</td>\n",
       "      <td>178.328069</td>\n",
       "      <td>20.558176</td>\n",
       "      <td>208.543834</td>\n",
       "      <td>23.564870</td>\n",
       "      <td>201.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212.601445</td>\n",
       "      <td>23.133033</td>\n",
       "      <td>211.282000</td>\n",
       "      <td>22.215456</td>\n",
       "      <td>192.070488</td>\n",
       "      <td>28.609185</td>\n",
       "      <td>214.756526</td>\n",
       "      <td>22.858145</td>\n",
       "      <td>225.100928</td>\n",
       "      <td>28.963123</td>\n",
       "      <td>176.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203.030371</td>\n",
       "      <td>28.911401</td>\n",
       "      <td>201.061282</td>\n",
       "      <td>23.613885</td>\n",
       "      <td>204.463873</td>\n",
       "      <td>24.207951</td>\n",
       "      <td>205.678831</td>\n",
       "      <td>22.870535</td>\n",
       "      <td>185.397177</td>\n",
       "      <td>23.550661</td>\n",
       "      <td>241.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186.804065</td>\n",
       "      <td>21.850135</td>\n",
       "      <td>213.228944</td>\n",
       "      <td>19.950735</td>\n",
       "      <td>192.241515</td>\n",
       "      <td>38.701339</td>\n",
       "      <td>193.741199</td>\n",
       "      <td>23.144597</td>\n",
       "      <td>186.612066</td>\n",
       "      <td>26.570859</td>\n",
       "      <td>270.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>197.146468</td>\n",
       "      <td>26.278929</td>\n",
       "      <td>184.391343</td>\n",
       "      <td>20.200295</td>\n",
       "      <td>196.451454</td>\n",
       "      <td>20.574849</td>\n",
       "      <td>208.121654</td>\n",
       "      <td>22.250040</td>\n",
       "      <td>225.065444</td>\n",
       "      <td>24.279930</td>\n",
       "      <td>170.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>216.218427</td>\n",
       "      <td>23.045336</td>\n",
       "      <td>184.406719</td>\n",
       "      <td>22.745005</td>\n",
       "      <td>205.679850</td>\n",
       "      <td>23.701416</td>\n",
       "      <td>216.084516</td>\n",
       "      <td>23.234242</td>\n",
       "      <td>202.463100</td>\n",
       "      <td>26.533995</td>\n",
       "      <td>204.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>190.222103</td>\n",
       "      <td>22.590183</td>\n",
       "      <td>174.759577</td>\n",
       "      <td>23.119709</td>\n",
       "      <td>204.210510</td>\n",
       "      <td>26.498276</td>\n",
       "      <td>211.692251</td>\n",
       "      <td>21.314292</td>\n",
       "      <td>184.198595</td>\n",
       "      <td>20.567987</td>\n",
       "      <td>141.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>203.323110</td>\n",
       "      <td>21.747052</td>\n",
       "      <td>209.113547</td>\n",
       "      <td>22.437475</td>\n",
       "      <td>192.202434</td>\n",
       "      <td>22.161219</td>\n",
       "      <td>175.282334</td>\n",
       "      <td>24.518261</td>\n",
       "      <td>214.732986</td>\n",
       "      <td>27.596598</td>\n",
       "      <td>225.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>204.333004</td>\n",
       "      <td>23.293121</td>\n",
       "      <td>210.865174</td>\n",
       "      <td>24.377930</td>\n",
       "      <td>172.536706</td>\n",
       "      <td>25.285008</td>\n",
       "      <td>195.839551</td>\n",
       "      <td>23.411689</td>\n",
       "      <td>174.075986</td>\n",
       "      <td>21.931337</td>\n",
       "      <td>179.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GCA_perm_1_pred  bmi_perm_1_pred  GCA_perm_2_pred  bmi_perm_2_pred   \n",
       "0         186.071931        24.135502       209.310151        24.111114  \\\n",
       "1         197.944629        24.089197       183.521658        24.904964   \n",
       "2         212.601445        23.133033       211.282000        22.215456   \n",
       "3         203.030371        28.911401       201.061282        23.613885   \n",
       "4         186.804065        21.850135       213.228944        19.950735   \n",
       "..               ...              ...              ...              ...   \n",
       "172       197.146468        26.278929       184.391343        20.200295   \n",
       "173       216.218427        23.045336       184.406719        22.745005   \n",
       "174       190.222103        22.590183       174.759577        23.119709   \n",
       "175       203.323110        21.747052       209.113547        22.437475   \n",
       "176       204.333004        23.293121       210.865174        24.377930   \n",
       "\n",
       "     GCA_perm_3_pred  bmi_perm_3_pred  GCA_perm_4_pred  bmi_perm_4_pred   \n",
       "0         204.221377        25.525347       200.576436        24.449453  \\\n",
       "1         187.247643        23.118162       178.328069        20.558176   \n",
       "2         192.070488        28.609185       214.756526        22.858145   \n",
       "3         204.463873        24.207951       205.678831        22.870535   \n",
       "4         192.241515        38.701339       193.741199        23.144597   \n",
       "..               ...              ...              ...              ...   \n",
       "172       196.451454        20.574849       208.121654        22.250040   \n",
       "173       205.679850        23.701416       216.084516        23.234242   \n",
       "174       204.210510        26.498276       211.692251        21.314292   \n",
       "175       192.202434        22.161219       175.282334        24.518261   \n",
       "176       172.536706        25.285008       195.839551        23.411689   \n",
       "\n",
       "     GCA_perm_5_pred  bmi_perm_5_pred  GCA_perm_1_real  bmi_perm_1_real   \n",
       "0         197.981595        22.436691            130.0             27.0  \\\n",
       "1         208.543834        23.564870            201.0             22.0   \n",
       "2         225.100928        28.963123            176.0             21.0   \n",
       "3         185.397177        23.550661            241.0             28.0   \n",
       "4         186.612066        26.570859            270.0             22.0   \n",
       "..               ...              ...              ...              ...   \n",
       "172       225.065444        24.279930            170.0             23.0   \n",
       "173       202.463100        26.533995            204.0             22.0   \n",
       "174       184.198595        20.567987            141.0             30.0   \n",
       "175       214.732986        27.596598            225.0             22.0   \n",
       "176       174.075986        21.931337            179.0             25.0   \n",
       "\n",
       "     GCA_perm_2_real  bmi_perm_2_real  GCA_perm_3_real  bmi_perm_3_real   \n",
       "0              270.0             20.0            256.0             22.0  \\\n",
       "1              180.0             23.0            197.0             23.0   \n",
       "2              128.0             20.0            233.0             24.0   \n",
       "3              203.0             25.0            198.0             24.0   \n",
       "4              236.0             26.0            105.0             47.0   \n",
       "..               ...              ...              ...              ...   \n",
       "172            155.0             26.0            138.0             17.0   \n",
       "173            167.0             24.0            203.0             27.0   \n",
       "174            218.0             30.0            193.0             23.0   \n",
       "175            236.0             22.0            131.0             24.0   \n",
       "176            185.0             21.0            165.0             19.0   \n",
       "\n",
       "     GCA_perm_4_real  bmi_perm_4_real  GCA_perm_5_real  bmi_perm_5_real  \n",
       "0              172.0             27.0            226.0             23.0  \n",
       "1               90.0             25.0            203.0             27.0  \n",
       "2              239.0             24.0            250.0             22.0  \n",
       "3              215.0             21.0            162.0             32.0  \n",
       "4              181.0             19.0            211.0             28.0  \n",
       "..               ...              ...              ...              ...  \n",
       "172            137.0             24.0            234.0             24.0  \n",
       "173            213.0             24.0            193.0             23.0  \n",
       "174            235.0             28.0            138.0             23.0  \n",
       "175            253.0             22.0            230.0             29.0  \n",
       "176            230.0             21.0            233.0             17.0  \n",
       "\n",
       "[177 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_real_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([226., 203., 250., 162., 211.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cogtest[4,0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = np.random.random((5,2))\n",
    "var = np.random.random((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08912571 0.33230529]\n",
      " [0.06010136 0.38565568]\n",
      " [0.07517595 0.49635083]\n",
      " [0.00390204 0.37530096]\n",
      " [0.03478023 0.51403657]]\n",
      "[10. inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kyb/agks/sheczko/.local/lib/python3.10/site-packages/numpy/core/function_base.py:284: RuntimeWarning: overflow encountered in power\n",
      "  return _nx.power(base, y)\n"
     ]
    }
   ],
   "source": [
    "print(r2)\n",
    "print(np.logspace(start = 1, stop = 20000, num = 50, endpoint=True, dtype=None, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GCA_r2</th>\n",
       "      <th>GCA_var</th>\n",
       "      <th>bmi_r2</th>\n",
       "      <th>bmi_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352930</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>0.946853</td>\n",
       "      <td>0.424036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.834027</td>\n",
       "      <td>0.237094</td>\n",
       "      <td>0.935719</td>\n",
       "      <td>0.028854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.292971</td>\n",
       "      <td>0.420059</td>\n",
       "      <td>0.623242</td>\n",
       "      <td>0.709984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.492207</td>\n",
       "      <td>0.558587</td>\n",
       "      <td>0.191360</td>\n",
       "      <td>0.190099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442167</td>\n",
       "      <td>0.563885</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>0.828950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GCA_r2   GCA_var    bmi_r2   bmi_var\n",
       "0  0.352930  0.190132  0.946853  0.424036\n",
       "1  0.834027  0.237094  0.935719  0.028854\n",
       "2  0.292971  0.420059  0.623242  0.709984\n",
       "3  0.492207  0.558587  0.191360  0.190099\n",
       "4  0.442167  0.563885  0.067993  0.828950"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
